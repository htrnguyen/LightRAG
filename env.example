### LightRAG Simplified Configuration
### This is a minimal configuration for local development

###########################
### Server Configuration
###########################
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='LightRAG Knowledge Base'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"

### Single worker for local development
WORKERS=1
TIMEOUT=150

### Directory Configuration
# INPUT_DIR=./data/inputs
# WORKING_DIR=./data/rag_storage
# TIKTOKEN_CACHE_DIR=./data/tiktoken

### Logging
# LOG_LEVEL=INFO
# VERBOSE=False

######################################################################################
### Query Configuration
######################################################################################
ENABLE_LLM_CACHE=true
# COSINE_THRESHOLD=0.2
# TOP_K=40
# CHUNK_TOP_K=20
# MAX_ENTITY_TOKENS=6000
# MAX_RELATION_TOKENS=8000
# MAX_TOTAL_TOKENS=30000

### Reranking (disabled by default)
RERANK_BINDING=null

########################################
### Document Processing
########################################
ENABLE_LLM_CACHE_FOR_EXTRACT=true
SUMMARY_LANGUAGE=English

### File upload size limit (100MB default)
# MAX_UPLOAD_SIZE=104857600

### Chunk size for document splitting
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

###############################
### Concurrency Configuration
###############################
MAX_ASYNC=2
MAX_PARALLEL_INSERT=1
# EMBEDDING_FUNC_MAX_ASYNC=4
# EMBEDDING_BATCH_NUM=10

###########################################################################
### LLM Configuration
### Supported: openai, ollama, azure_openai, aws_bedrock, gemini
###########################################################################
LLM_BINDING=openai
LLM_MODEL=gpt-4o-mini
LLM_BINDING_HOST=https://api.openai.com/v1
LLM_BINDING_API_KEY=your_openai_api_key_here

### Ollama example (uncomment to use)
# LLM_BINDING=ollama
# LLM_MODEL=qwen2.5:32b
# LLM_BINDING_HOST=http://localhost:11434
# OLLAMA_LLM_NUM_CTX=32768

### Gemini example (uncomment to use)
# LLM_BINDING=gemini
# LLM_MODEL=gemini-2.0-flash
# LLM_BINDING_API_KEY=your_gemini_api_key
# LLM_BINDING_HOST=https://generativelanguage.googleapis.com

#######################################################################################
### Embedding Configuration
### IMPORTANT: Do not change after first file is processed!
#######################################################################################
EMBEDDING_BINDING=openai
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIM=3072
EMBEDDING_TOKEN_LIMIT=8192
EMBEDDING_BINDING_HOST=https://api.openai.com/v1
EMBEDDING_BINDING_API_KEY=your_openai_api_key_here

### Ollama embedding example (uncomment to use)
# EMBEDDING_BINDING=ollama
# EMBEDDING_MODEL=bge-m3:latest
# EMBEDDING_DIM=1024
# EMBEDDING_BINDING_HOST=http://localhost:11434

### Gemini embedding example (uncomment to use)
# EMBEDDING_BINDING=gemini
# EMBEDDING_MODEL=text-embedding-004
# EMBEDDING_DIM=768
# EMBEDDING_SEND_DIM=true
# EMBEDDING_BINDING_HOST=https://generativelanguage.googleapis.com
# EMBEDDING_BINDING_API_KEY=your_gemini_api_key

############################
### Storage Configuration
############################
### Using local file-based storage (default)
LIGHTRAG_KV_STORAGE=JsonKVStorage
LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage
LIGHTRAG_GRAPH_STORAGE=NetworkXStorage
LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage

### No workspace isolation (single workspace)
# WORKSPACE=
